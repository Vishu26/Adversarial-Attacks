# Adversarial-Attacks

## Nontargeted Adversarial Attacks

The goal of the non-targeted attack is to slightly modify source image in a way that image will be classified incorrectly by generally unknown machine learning classifier.

## Targeted Adversarial Attacks

The goal of the targeted attack is to slightly modify source image in a way that image will be classified as specified target class by generally unknown machine learning classifier.

## Defense Against Adversarial Attack

The goal of the defense is to build machine learning classifier which is robust to adversarial example, i.e. can classify adversarial images correctly.

## References

### Adversarial Examples: Attacks and Defenses for Deep Learning - [pdf](https://arxiv.org/pdf/1712.07107.pdf)
### Practical Black-Box Attacks against Machine Learning - [pdf](https://arxiv.org/pdf/1602.02697.pdf)

## Examples

![alt text](Example/ex0.png)
![alt text](Example/ex1.png)
![alt text](Example/ex2.png)
![alt text](Example/ex3.png)
![alt text](Example/ex4.png)
